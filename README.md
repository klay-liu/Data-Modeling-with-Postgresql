## Project01: Data Modeling with Postgres

## Table of contents
* [Introduction](##introduction)
* [Data](#data)
* [Technologies](#technologies)
* [Final Tables](#finaltable)


## Introduction

To provide easier ways to query and analyze the songs data that users listen to from the organized database for analytical team in startup 'Sparkify', a Postgres database with star schema from two JSON directories is created. 
For database schema design, we store the measurements related fields the analytical team needed in a fact table, which is the songplays table in this project. other categorical info for users, songs can be stored in dimensional tables.
We extract data from two directories the Sparkify collected and stored, transform, insert the processed data into appropriate tables with Python.

## Data
### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
data/song_data/A/B/C/TRABCEI128F424C983.json
data/song_data/A/A/B/TRAABJL12903CDCF1A.json
```

### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate app activity logs from a music streaming app based on specified configurations.
The log files in the dataset we'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.

```
data/log_data/2018/11/2018-11-12-events.json
data/log_data/2018/11/2018-11-13-events.json
```

## Technologies
- Postgresql
    - Create database and tables(songplays, songs, users, artists, time) (scripts: sql_queries.py; create_tables.py)
    
- ETL with Python3
    - Extract: Extract data from song_file directory and log_file directory with Python (script: etl.py)
    - Transform: Transform the values for target fields of tables with Pandas DataFrame (script: etl.py)
    - Load: Insert the transformed data into tables by excuting insert query with Python (script: etl.py)

    Exctuted order for scripts: sql_queries.py --> create_tables.py --> etl.py


## Final Tables


The star schema for database:

**Fact Table**:
songplays

**Dimension Tables**:
users
songs
artists
time

![image](https://user-images.githubusercontent.com/30792411/100759611-c77c9200-342b-11eb-96b8-4097183a52f4.png)

